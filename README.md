# LLM Bias Amplification and Mitigation
This project investigates the tendency of Large Language Models (LLMs) to amplify societal biases during multi-step reasoning. It focuses on identifying how these biases manifest in complex tasks and evaluates the effectiveness of different mitigation strategies.

### Core Objectives
Bias Identification: Developing frameworks to detect and quantify gender, racial, and professional biases within model outputs.

Amplification Analysis: Researching whether models intensify existing data biases when performing sequential or chain-of-thought reasoning.

Mitigation Techniques: Testing various approaches to reduce biased behavior, including prompt engineering and specialized evaluation benchmarks.
